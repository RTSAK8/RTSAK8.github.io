<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Optimizers - Principles and Strategies</title>
      <link href="/2023/12/18/Optimizer-principles%20and%20strategies/"/>
      <url>/2023/12/18/Optimizer-principles%20and%20strategies/</url>
      
        <content type="html"><![CDATA[<h2 id="Gradient-Descent-and-Optimization"><a href="#Gradient-Descent-and-Optimization" class="headerlink" title="Gradient Descent and Optimization"></a>Gradient Descent and Optimization</h2><h3 id="Shortcomings-of-Gradient-Descent"><a href="#Shortcomings-of-Gradient-Descent" class="headerlink" title="Shortcomings of Gradient Descent"></a>Shortcomings of Gradient Descent</h3><p>Using gradient descent to train neural networks is arguably the most effective method. However, in practice, traditional gradient descent is quite challenging to implement due to the substantial computational resources and time it requires. In deep learning, we need to optimize gradient descent to achieve similar or approximate results with less computation.</p><p>Generally, there are two directions for optimizing gradient descent:</p><ul><li>Optimizing network structure, such as Max Pool layers, Dropout layers, Convolutional layers, etc.</li><li>Adjusting optimization strategies.</li></ul><p>The first method of optimization needs to be designed according to the specific network and task. Here, we focus on the second method, using optimizers to adjust the strategy of gradient descent.</p><h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><p>In Pytorch, many optimizers are prepared for us under <code>torch.optim</code>. Here, we choose a few representative optimizers to explain their operating mechanisms.</p><h2 id="Introduction-to-Optimizers"><a href="#Introduction-to-Optimizers" class="headerlink" title="Introduction to Optimizers"></a>Introduction to Optimizers</h2><h3 id="SGD-Stochastic-Gradient-Descent"><a href="#SGD-Stochastic-Gradient-Descent" class="headerlink" title="SGD (Stochastic Gradient Descent)"></a>SGD (Stochastic Gradient Descent)</h3><p>In traditional gradient descent, we need to input all data into the network for computation and perform gradient descent based on the structure. However, due to computer performance limitations, this method is impractical for massive data; a computer cannot read in large amounts of data at once.</p><p>In fact, after expectation calculations and verification, people found that it’s not necessary to read all data for a single gradient descent. It’s sufficient to select a batch of data for gradient descent. By continuous training, the neural network can achieve results not much different from performing gradient descent on all data. This is also a basic principle of batch training.</p><p>The SGD optimizer is located in <code>torch.optim.SGD</code> and is currently the most commonly used optimizer in deep learning.</p><h4 id="Mathematical-Expression"><a href="#Mathematical-Expression" class="headerlink" title="Mathematical Expression"></a>Mathematical Expression</h4><p>The formula for SGD is no different from the original gradient descent:<br>$$<br>W_t \leftarrow W_t - lr * \frac{\partial f(W_{t-1})}{\partial W}<br>$$</p><h4 id="Parameter-Description"><a href="#Parameter-Description" class="headerlink" title="Parameter Description"></a>Parameter Description</h4><ul><li><code>params</code>: Parameters to be trained</li><li><code>lr</code>: Learning rate</li><li><code>momentum</code>: Momentum parameter (discussed later)</li></ul><h3 id="Momentum-Method"><a href="#Momentum-Method" class="headerlink" title="Momentum Method"></a>Momentum Method</h3><p>Momentum is more a strategy than an optimizer, and many optimizers later are designed based on momentum. The official Pytorch SGD optimizer also has a corresponding momentum optimization scheme.</p><p>During training, when approaching the optimal value, the gradient becomes smaller. Due to the fixed learning rate, the convergence speed of ordinary gradient descent slows down, either oscillating around the optimum point or getting stuck in a local optimum.</p><p>Momentum takes inspiration from the concept of momentum in physics. For the current gradient descent, we consider the “past” state, allowing the gradient of the past moment to affect the current descent strategy.</p><p>Imagine, if the gradient direction at the current moment is the same as that of the previous moment, then we are likely to advance a long distance on this segment. At this time, we can appropriately accelerate to get out of the plateau or local optimum area faster.</p><h4 id="Mathematical-Expression-1"><a href="#Mathematical-Expression-1" class="headerlink" title="Mathematical Expression"></a>Mathematical Expression</h4><p>$$<br>W_t \leftarrow W_{t-1} - lr*V_t \<br>V_t = V_{t-1} + \Delta W_t \<br>\Delta W_t = \frac{\partial J(W_{t-1})}{\partial W}<br>$$</p><p>In practice, the formula is optimized as:<br>$$<br>V_t=\beta * V_{t-1} + (1-\beta)*\Delta W_t<br>$$</p><ul><li>$\beta$ is a parameter between 0 and 1</li></ul><p>The advantage of this optimization is that it not only adjusts the current gradient with a coefficient but also reduces the weight of too outdated information.</p><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>Momentum method makes adjustments on the parameter $W$ that needs to be updated. Can we also make adjustments to the learning rate $lr$? AdaGrad adopts this idea for optimization.</p><p>AdaGrad is located in <code>torch.optim.Adagrad</code>.</p><h4 id="Mathematical-Expression-2"><a href="#Mathematical-Expression-2" class="headerlink" title="Mathematical Expression"></a>Mathematical Expression</h4><p>$$<br>W_t \leftarrow W_{t-1} - \frac{lr}{\sqrt{S_t}+\mu}*W_{t-1} \<br>S_t = S_{t-1}+\Delta{W_t^TW_t}<br>$$</p><ul><li>$\mu$ is a small parameter for adjusting the learning rate.</li><li>$S$ and $V$ are similar, both adjusting $lr$ using historical data. Similarly, the calculation formula for $S$ can also be optimized:</li></ul><p>$$<br>S_t=\beta * S_{t-1} + (1-\beta)*\Delta{W_t^TW}<br>$$</p><h4 id="Parameter-Description-1"><a href="#Parameter-Description-1" class="headerlink" title="Parameter Description"></a>Parameter Description</h4><ul><li><code>params</code>: Parameters to be trained</li><li><code>lr</code>: Learning rate</li><li>`eps</li></ul><p>`: Represents $\mu$</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam is the most commonly used optimizer in recent years, with its research paper citations exceeding 100,000, highlighting its significance in deep learning.</p><p>To put it simply, the Adam method combines the momentum method and AdaGrad, considering both the correction of $S$ to $lr$ and the correction of $V$ to $W$.</p><p>Adam is located at <code>torch.optim.Adam</code>.</p><h4 id="Mathematical-Expression-3"><a href="#Mathematical-Expression-3" class="headerlink" title="Mathematical Expression"></a>Mathematical Expression</h4><p>$$<br>W_t \leftarrow W_{t-1} - \frac{lr}{\sqrt{S_t}+\mu}*V_t<br>$$</p><p>This formula does not require much explanation, as it essentially combines the above formulas.</p><h4 id="Parameter-Description-2"><a href="#Parameter-Description-2" class="headerlink" title="Parameter Description"></a>Parameter Description</h4><ul><li><code>params</code>: Parameters to be trained</li><li><code>lr</code>: Learning rate</li><li><code>betas</code>: A pair of tuples, $(\beta1,\beta2)$, representing the parameters for $S$ and $V$ respectively</li><li><code>eps</code>: Represents $\mu$</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
